# Talos Kubernetes sur Outscale

[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Talos](https://img.shields.io/badge/Talos-v1.11.3-blue.svg)](https://www.talos.dev/)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-v1.34-blue.svg)](https://kubernetes.io/)

D√©ploiement automatis√© d'un cluster Kubernetes hautement disponible avec **Talos Linux** sur le cloud **Outscale**.

Ce projet construis une infrastructure compl√®te pour ex√©cuter Kubernetes sur Outscale avec :

- **Talos Linux** : OS immuable et API-driven pour Kubernetes
- **Terraform** : Infrastructure as Code pour le provisioning
- **Packer** : Automatisation de la cr√©ation d'images OMI personnalis√©es (standard et GPU)
- **Cilium** : CNI moderne bas√© sur eBPF avec kube-proxy replacement
- **Support GPU** : Workers GPU avec drivers NVIDIA pour workloads d'IA/ML
- **CSI Driver** : Stockage persistant avec volumes BSU Outscale
- **Cloud Controller Manager** : Int√©gration native avec Load Balancers Outscale

## üìã Table des mati√®res

* [Talos Kubernetes sur Outscale](#talos-kubernetes-sur-outscale)
  * [üìã Table des mati√®res](#-table-des-mati√®res)
  * [üèó Architecture](#-architecture)
    * [Topologie r√©seau](#topologie-r√©seau)
    * [Composants](#composants)
  * [üîß Pr√©requis](#-pr√©requis)
    * [Outils requis](#outils-requis)
    * [Sur votre poste de travail](#sur-votre-poste-de-travail)
    * [Sur le bastion](#sur-le-bastion)
    * [Versions test√©es](#versions-test√©es)
  * [üöÄ D√©marrage rapide](#-d√©marrage-rapide)
    * [1. Configuration des credentials](#1-configuration-des-credentials)
    * [2. Cr√©ation des OMI Talos](#2-cr√©ation-des-omi-talos)
      * [Image Talos Standard](#image-talos-standard)
      * [Image Talos GPU (optionnel)](#image-talos-gpu-optionnel)
    * [3. D√©ploiement de l'infrastructure](#3-d√©ploiement-de-linfrastructure)
    * [4. D√©ploiement du cluster Kubernetes](#4-d√©ploiement-du-cluster-kubernetes)
  * [ÔøΩ Documentation](#-documentation)
    * [Fonctionnalit√©s principales](#fonctionnalit√©s-principales)
      * [Support GPU NVIDIA](#support-gpu-nvidia)
      * [CSI Driver Outscale](#csi-driver-outscale)
      * [Cloud Controller Manager](#cloud-controller-manager)
  * [ü§ù Contribution](#-contribution)
    * [Guidelines](#guidelines)
  * [üìù Licence](#-licence)
  * [üë§ Auteur](#-auteur)

## üèó Architecture

### Topologie r√©seau

L'infrastructure repose sur une architecture multi-AZ hautement disponible :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         Outscale Cloud (eu-west-2)                                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Net Bastion (10.100.0.0/16) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                 ‚îÇ
‚îÇ  ‚îÇ                                                 ‚îÇ                                 ‚îÇ
‚îÇ  ‚îÇ  Bastion (10.100.1.10)                          ‚îÇ                                 ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ SSH Access (Port 22)                        ‚îÇ                                 ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ kubectl, talosctl, helm                     ‚îÇ                                 ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ VPC Peering ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ                       ‚îÇ
‚îÇ                                                              ‚ñº                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Net Cluster Kubernetes (10.0.0.0/16) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ                                                                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  AZ-A (.1.0/24)  ‚îÇ  ‚îÇ  AZ-B (.2.0/24)  ‚îÇ  ‚îÇ  AZ-C (.3.0/24)  ‚îÇ              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ CP-1: .1.10      ‚îÇ  ‚îÇ CP-2: .2.11      ‚îÇ  ‚îÇ CP-3: .3.12      ‚îÇ              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Worker-1: .1.20  ‚îÇ  ‚îÇ Worker-2: .2.21  ‚îÇ  ‚îÇ Worker-3: .3.22  ‚îÇ              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ GPU-1: .1.30     ‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ                  ‚îÇ              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ NAT Subnet (.254.0/24) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  NAT Gateway (Public IP)                    ‚îÇ  ‚Üí Internet                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Load Balancer (Internal) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Kubernetes API (Port 6443)                 ‚îÇ                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ> 10.0.1.10:6443 (CP-1)                  ‚îÇ                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ> 10.0.2.11:6443 (CP-2)                  ‚îÇ                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ> 10.0.3.12:6443 (CP-3)                  ‚îÇ                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                                                ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Composants

**R√©seau**

- CNI : Cilium (eBPF, native routing mode)
- Pod CIDR : `10.244.0.0/16`
- Service CIDR : `10.96.0.0/12`
- kube-proxy : D√©sactiv√© (remplac√© par Cilium)
- MTU : 9001 (jumbo frames Outscale)

**Control Plane (3 n≈ìuds)**

- Distribution : 3 zones de disponibilit√© (AZ-1, AZ-2, AZ-3)
- Type : `tinav6.c4r8p2` (4 vCPU, 8 GB RAM)
- R√¥les : etcd, kube-apiserver, kube-controller-manager, kube-scheduler
- HA : Quorum etcd 3 n≈ìuds (tol√®re 1 panne)

**Workers (2+ n≈ìuds)**

- Distribution : 2 zones de disponibilit√© minimum
- Type : Configurable selon les charges applicatives
- R√¥les : H√©bergement des workloads Kubernetes

**Workers GPU (optionnels)**

- Distribution : Zones de disponibilit√© configurables
- Type : `tinav6.c8r16p1` ou sup√©rieur (avec line GPU)
- Image : OMI Talos personnalis√©e avec drivers NVIDIA
- Extensions : nvidia-open-gpu-kernel-modules, nvidia-container-toolkit, nvidia-fabricmanager
- Drivers : NVIDIA 570.x (production)
- Support : GPU simple, multi-GPU, et syst√®mes HGX avec NVLink
- R√¥les : Workloads IA/ML, calcul scientifique, training de mod√®les

**Stockage**

- Type : BSU volumes (gp2, io1, standard)
- Taille : Configurable par n≈ìud
- Persistence : DeleteOnVmDeletion configurable

## üîß Pr√©requis

### Outils requis

Sur votre poste de travail et le bastion :

### Sur votre poste de travail

```bash
# Terraform (>= 1.0)
terraform version

# Packer (>= 1.9)
packer version

# osc-cli (CLI Outscale)
osc-cli --version
```

### Sur le bastion

```bash
# talosctl (m√™me version que les images Talos)
talosctl version

# kubectl
kubectl version --client

# helm
helm version
```

### Versions test√©es

| Composant | Version |
|-----------|---------|
| Talos Linux | v1.11.5 |
| Kubernetes | v1.34.1 |
| Cilium | v1.18.4 |
| Terraform | v1.9+ |
| Packer | v1.11+ |

## üöÄ D√©marrage rapide

### 1. Configuration des credentials

Clonez le repository et configurez vos credentials :

```bash
git clone https://github.com/stephrobert/talos-outscale.git
cd talos-outscale

# Copier le fichier d'exemple
cp .envrc.sample .envrc

# √âditer avec vos credentials
vim .envrc
```

Contenu de `.envrc` :

```bash
export OSC_ACCESS_KEY="VOTRE_ACCESS_KEY"
export OSC_SECRET_KEY="VOTRE_SECRET_KEY"
export OSC_REGION="eu-west-2"

export TF_VAR_access_key_id="$OSC_ACCESS_KEY"
export TF_VAR_secret_key_id="$OSC_SECRET_KEY"

export PACKER_LOG=1
export PACKER_LOG_PATH="./packer.log"
```

Chargez les variables :

```bash
source .envrc
# Ou avec direnv
direnv allow
```

### 2. Cr√©ation des OMI Talos

Le projet supporte deux types d'images :

#### Image Talos Standard

Pour les n≈ìuds sans GPU (control planes et workers CPU) :

```bash
cd packer

# Copier et √©diter les variables
cp variables.auto.pkrvars.hcl.example variables.auto.pkrvars.hcl
vim variables.auto.pkrvars.hcl

# Initialiser Packer
packer init talos-outscale.pkr.hcl

# Valider la configuration
packer validate talos-outscale.pkr.hcl

# Build de l'OMI standard
packer build talos-outscale.pkr.hcl
```

#### Image Talos GPU (optionnel)

Pour les workers GPU avec drivers NVIDIA int√©gr√©s :

```bash
cd packer

# Initialiser si pas encore fait
packer init talos-gpu-outscale.pkr.hcl

# Valider la configuration GPU
packer validate talos-gpu-outscale.pkr.hcl

# Build de l'OMI GPU (type universal recommand√©)
packer build talos-gpu-outscale.pkr.hcl
```

**Types d'images GPU disponibles :**

- **universal** (par d√©faut) : Inclut `nvidia-fabricmanager` pour support NVLink/HGX (H100, A100)

Les images cr√©√©es auront des noms comme :

- Standard : `Talos-v1.11.5-20251126-093750`
- GPU : `Talos-GPU-universal-v1.11.5-20251126-100029`

### 3. D√©ploiement de l'infrastructure

```bash
cd terraform

# Copier le fichier d'exemple de variables
cp terraform.tfvars.example terraform.tfvars

# √âditer avec vos param√®tres
# - Mettre l'OMI ID standard cr√©√©e pr√©c√©demment
# - Configurer le nombre de workers GPU (gpu_worker_count)

vim terraform.tfvars

# Initialiser Terraform
terraform init

# Planifier les changements
terraform plan

# Appliquer
terraform apply
```

**Configuration GPU dans Terraform :**

Variables disponibles pour les workers GPU :

- `gpu_worker_count` : Nombre de workers GPU (d√©faut: 0)
- `gpu_worker_vm_type` : Type de VM avec GPU (ex: `tinav6.c8r16p1`)
- `gpu_worker_disk_size` : Taille du disque en GB (d√©faut: 200)
- `gpu_worker_availability_zones` : Liste des AZ pour GPU (ex: `["a"]`)
- `talos_gpu_image_id` : ID de l'OMI GPU cr√©√©e avec Packer

### 4. D√©ploiement du cluster Kubernetes

Pour le bootstrap du cluster Kubernetes et l'installation de Cilium, consultez le **guide complet de d√©ploiement** :

üìñ **[Guide de d√©ploiement Talos sur Outscale](https://blog.stephane-robert.info/docs/cloud/outscale/kubernetes-talos/)**

Ce guide d√©taille :

- La g√©n√©ration des configurations Talos
- Le bootstrap du cluster etcd
- L'installation et la configuration de Cilium CNI
- Les tests de connectivit√©
- Le troubleshooting

üéâ **Votre cluster Kubernetes Talos sera op√©rationnel !**

## üìö Documentation

Documentation disponible :

- **Guide complet de d√©ploiement** : [D√©ploiement Talos sur Outscale](https://blog.stephane-robert.info/docs/cloud/outscale/kubernetes-talos/)

Ressources externes :

- **Talos officiel** : <https://www.talos.dev/>
- **Cilium** : <https://docs.cilium.io/>
- **Terraform Outscale** : <https://registry.terraform.io/providers/outscale/outscale/>

### Fonctionnalit√©s principales

#### Support GPU NVIDIA

Le projet supporte le d√©ploiement de workers GPU avec :

- **Images personnalis√©es** : OMI Talos avec drivers NVIDIA pr√©-install√©s
- **Extensions Talos** : nvidia-open-gpu-kernel-modules, nvidia-container-toolkit, nvidia-fabricmanager
- **Drivers** : NVIDIA 570.x (production)
- **Configurations** : GPU simple, multi-GPU, et syst√®mes HGX avec NVLink
- **GPU Operator** : Installation et gestion automatis√©es des composants GPU

#### CSI Driver Outscale

Support du stockage persistant avec volumes BSU :

- **StorageClass** : gp2, io1, standard
- **Dynamic provisioning** : Cr√©ation automatique de volumes
- **Volume expansion** : Redimensionnement √† chaud
- **Snapshots** : Sauvegarde et restauration

#### Cloud Controller Manager

Int√©gration native avec les services Outscale :

- **Load Balancers** : Cr√©ation automatique de LB pour les Services Kubernetes
- **Node management** : Synchronisation des m√©tadonn√©es VM/Node
- **Zone awareness** : Distribution multi-AZ intelligente

## ü§ù Contribution

Les contributions sont les bienvenues ! N'h√©sitez pas √† :

1. Forker le projet
2. Cr√©er une branche feature (`git checkout -b feature/AmazingFeature`)
3. Commiter vos changements (`git commit -m 'Add some AmazingFeature'`)
4. Pousser vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

### Guidelines

- Respectez la structure du projet
- Documentez les nouveaux composants
- Testez sur une infrastructure de dev avant de proposer
- Mettez √† jour le README si n√©cessaire

## üìù Licence

Ce projet est distribu√© sous licence Apache 2.0. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

## üë§ Auteur

**St√©phane Robert**

- Blog: <https://blog.stephane-robert.info>
- GitHub: [@stephrobert](https://github.com/stephrobert)
- LinkedIn: [St√©phane Robert](https://www.linkedin.com/in/stephanerobert1/)

---

‚≠ê **Si ce projet vous est utile, n'h√©sitez pas √† lui mettre une star !**
