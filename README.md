# Talos Kubernetes sur Outscale

[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Talos](https://img.shields.io/badge/Talos-v1.11.3-blue.svg)](https://www.talos.dev/)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-v1.34-blue.svg)](https://kubernetes.io/)

DÃ©ploiement automatisÃ© d'un cluster Kubernetes hautement disponible avec **Talos Linux** sur le cloud **Outscale**.

Ce projet construis une infrastructure complÃ¨te pour exÃ©cuter Kubernetes sur Outscale avec :

- **Talos Linux** : OS immuable et API-driven pour Kubernetes
- **Terraform** : Infrastructure as Code pour le provisioning
- **Packer** : Automatisation de la crÃ©ation d'images OMI personnalisÃ©es (standard et GPU)
- **Cilium** : CNI moderne basÃ© sur eBPF avec kube-proxy replacement
- **Support GPU** : Workers GPU avec drivers NVIDIA pour workloads d'IA/ML
- **CSI Driver** : Stockage persistant avec volumes BSU Outscale
- **Cloud Controller Manager** : IntÃ©gration native avec Load Balancers Outscale

## ğŸ“‹ Table des matiÃ¨res

* [Talos Kubernetes sur Outscale](#talos-kubernetes-sur-outscale)
  * [ğŸ“‹ Table des matiÃ¨res](#-table-des-matiÃ¨res)
  * [ğŸ— Architecture](#-architecture)
    * [Topologie rÃ©seau](#topologie-rÃ©seau)
    * [Composants](#composants)
  * [ğŸ”§ PrÃ©requis](#-prÃ©requis)
    * [Outils requis](#outils-requis)
    * [Sur votre poste de travail](#sur-votre-poste-de-travail)
    * [Sur le bastion](#sur-le-bastion)
    * [Versions testÃ©es](#versions-testÃ©es)
  * [ğŸš€ DÃ©marrage rapide](#-dÃ©marrage-rapide)
    * [1. Configuration des credentials](#1-configuration-des-credentials)
    * [2. CrÃ©ation des OMI Talos](#2-crÃ©ation-des-omi-talos)
      * [Image Talos Standard](#image-talos-standard)
      * [Image Talos GPU (optionnel)](#image-talos-gpu-optionnel)
    * [3. DÃ©ploiement de l'infrastructure](#3-dÃ©ploiement-de-linfrastructure)
    * [4. DÃ©ploiement du cluster Kubernetes](#4-dÃ©ploiement-du-cluster-kubernetes)
  * [ğŸ“ Structure du projet](#-structure-du-projet)
  * [ğŸ“š Documentation](#-documentation)
    * [FonctionnalitÃ©s principales](#fonctionnalitÃ©s-principales)
      * [Support GPU NVIDIA](#support-gpu-nvidia)
      * [CSI Driver Outscale](#csi-driver-outscale)
      * [Cloud Controller Manager](#cloud-controller-manager)
  * [ğŸ¤ Contribution](#-contribution)
    * [Guidelines](#guidelines)
  * [ğŸ“ Licence](#-licence)
  * [ğŸ‘¤ Auteur](#-auteur)

## ğŸ— Architecture

### Topologie rÃ©seau

L'infrastructure repose sur une architecture multi-AZ hautement disponible :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Outscale Cloud (eu-west-2)                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Net Bastion (10.100.0.0/16) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚  â”‚                                                 â”‚                                 â”‚
â”‚  â”‚  Bastion (10.100.1.10)                          â”‚                                 â”‚
â”‚  â”‚  â”œâ”€ SSH Access (Port 22)                        â”‚                                 â”‚
â”‚  â”‚  â”œâ”€ kubectl, talosctl, helm                     â”‚                                 â”‚
â”‚  â”‚  â””â”€ VPC Peering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚                       â”‚
â”‚                                                              â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Net Cluster Kubernetes (10.0.0.0/16) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                                                â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚
â”‚  â”‚  â”‚  AZ-A (.1.0/24)  â”‚  â”‚  AZ-B (.2.0/24)  â”‚  â”‚  AZ-C (.3.0/24)  â”‚              â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”‚  â”‚
â”‚  â”‚  â”‚ CP-1: .1.10      â”‚  â”‚ CP-2: .2.11      â”‚  â”‚ CP-3: .3.12      â”‚              â”‚  â”‚
â”‚  â”‚  â”‚ Worker-1: .1.20  â”‚  â”‚ Worker-2: .2.21  â”‚  â”‚ Worker-3: .3.22  â”‚              â”‚  â”‚
â”‚  â”‚  â”‚ GPU-1: .1.30     â”‚  â”‚                  â”‚  â”‚                  â”‚              â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚
â”‚  â”‚                                                                                â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ NAT Subnet (.254.0/24) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚  â”‚
â”‚  â”‚  â”‚  NAT Gateway (Public IP)                    â”‚  â†’ Internet                   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚  â”‚
â”‚  â”‚                                                                                â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€ Load Balancer (Internal) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚  â”‚
â”‚  â”‚  â”‚  Kubernetes API (Port 6443)                 â”‚                               â”‚  â”‚
â”‚  â”‚  â”‚  â”œâ”€> 10.0.1.10:6443 (CP-1)                  â”‚                               â”‚  â”‚
â”‚  â”‚  â”‚  â”œâ”€> 10.0.2.11:6443 (CP-2)                  â”‚                               â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€> 10.0.3.12:6443 (CP-3)                  â”‚                               â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚  â”‚
â”‚  â”‚                                                                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Composants

**RÃ©seau**

- CNI : Cilium (eBPF, native routing mode)
- Pod CIDR : `10.244.0.0/16`
- Service CIDR : `10.96.0.0/12`
- kube-proxy : DÃ©sactivÃ© (remplacÃ© par Cilium)
- MTU : 9001 (jumbo frames Outscale)

**Control Plane (3 nÅ“uds)**

- Distribution : 3 zones de disponibilitÃ© (AZ-1, AZ-2, AZ-3)
- Type : `tinav6.c4r8p2` (4 vCPU, 8 GB RAM)
- RÃ´les : etcd, kube-apiserver, kube-controller-manager, kube-scheduler
- HA : Quorum etcd 3 nÅ“uds (tolÃ¨re 1 panne)

**Workers (2+ nÅ“uds)**

- Distribution : 2 zones de disponibilitÃ© minimum
- Type : Configurable selon les charges applicatives
- RÃ´les : HÃ©bergement des workloads Kubernetes

**Workers GPU (optionnels)**

- Distribution : Zones de disponibilitÃ© configurables
- Type : `tinav6.c8r16p1` ou supÃ©rieur (avec line GPU)
- Image : OMI Talos personnalisÃ©e avec drivers NVIDIA
- Extensions : nvidia-open-gpu-kernel-modules, nvidia-container-toolkit, nvidia-fabricmanager
- Drivers : NVIDIA 570.x (production)
- Support : GPU simple, multi-GPU, et systÃ¨mes HGX avec NVLink
- RÃ´les : Workloads IA/ML, calcul scientifique, training de modÃ¨les

**Stockage**

- Type : BSU volumes (gp2, io1, standard)
- Taille : Configurable par nÅ“ud
- Persistence : DeleteOnVmDeletion configurable

## ğŸ”§ PrÃ©requis

### Outils requis

Sur votre poste de travail et le bastion :

### Sur votre poste de travail

```bash
# Terraform (>= 1.0)
terraform version

# Packer (>= 1.9)
packer version

# osc-cli (CLI Outscale)
osc-cli --version
```

### Sur le bastion

```bash
# talosctl (mÃªme version que les images Talos)
talosctl version

# kubectl
kubectl version --client

# helm
helm version
```

### Versions testÃ©es

| Composant | Version |
|-----------|---------|
| Talos Linux | v1.11.5 |
| Kubernetes | v1.34.1 |
| Cilium | v1.18.4 |
| Terraform | v1.9+ |
| Packer | v1.11+ |

## ğŸš€ DÃ©marrage rapide

### 1. Configuration des credentials

Clonez le repository et configurez vos credentials :

```bash
git clone https://github.com/stephrobert/talos-outscale.git
cd talos-outscale

# Copier le fichier d'exemple
cp .envrc.sample .envrc

# Ã‰diter avec vos credentials
vim .envrc
```

Contenu de `.envrc` :

```bash
export OSC_ACCESS_KEY="VOTRE_ACCESS_KEY"
export OSC_SECRET_KEY="VOTRE_SECRET_KEY"
export OSC_REGION="eu-west-2"

export TF_VAR_access_key_id="$OSC_ACCESS_KEY"
export TF_VAR_secret_key_id="$OSC_SECRET_KEY"

export PACKER_LOG=1
export PACKER_LOG_PATH="./packer.log"
```

Chargez les variables :

```bash
source .envrc
# Ou avec direnv
direnv allow
```

### 2. CrÃ©ation des OMI Talos

Le projet supporte deux types d'images :

#### Image Talos Standard

Pour les nÅ“uds sans GPU (control planes et workers CPU) :

```bash
cd packer

# Copier et Ã©diter les variables
cp variables.auto.pkrvars.hcl.example variables.auto.pkrvars.hcl
vim variables.auto.pkrvars.hcl

# Initialiser Packer
packer init talos-outscale.pkr.hcl

# Valider la configuration
packer validate talos-outscale.pkr.hcl

# Build de l'OMI standard
packer build talos-outscale.pkr.hcl
```

#### Image Talos GPU (optionnel)

Pour les workers GPU avec drivers NVIDIA intÃ©grÃ©s :

```bash
cd packer

# Initialiser si pas encore fait
packer init talos-gpu-outscale.pkr.hcl

# Valider la configuration GPU
packer validate talos-gpu-outscale.pkr.hcl

# Build de l'OMI GPU (type universal recommandÃ©)
packer build talos-gpu-outscale.pkr.hcl
```

**Types d'images GPU disponibles :**

- **universal** (par dÃ©faut) : Inclut `nvidia-fabricmanager` pour support NVLink/HGX (H100, A100)

Les images crÃ©Ã©es auront des noms comme :

- Standard : `Talos-v1.11.5-20251126-093750`
- GPU : `Talos-GPU-universal-v1.11.5-20251126-100029`

### 3. DÃ©ploiement de l'infrastructure

```bash
cd terraform

# Copier le fichier d'exemple de variables
cp terraform.tfvars.example terraform.tfvars

# Ã‰diter avec vos paramÃ¨tres
# - Mettre l'OMI ID standard crÃ©Ã©e prÃ©cÃ©demment
# - Configurer le nombre de workers GPU (gpu_worker_count)

vim terraform.tfvars

# Initialiser Terraform
terraform init

# Planifier les changements
terraform plan

# Appliquer
terraform apply
```

**Configuration GPU dans Terraform :**

Variables disponibles pour les workers GPU :

- `gpu_worker_count` : Nombre de workers GPU (dÃ©faut: 0)
- `gpu_worker_vm_type` : Type de VM avec GPU (ex: `tinav6.c8r16p1`)
- `gpu_worker_disk_size` : Taille du disque en GB (dÃ©faut: 200)
- `gpu_worker_availability_zones` : Liste des AZ pour GPU (ex: `["a"]`)
- `talos_gpu_image_id` : ID de l'OMI GPU crÃ©Ã©e avec Packer

### 4. DÃ©ploiement du cluster Kubernetes

Pour le bootstrap du cluster Kubernetes et l'installation de Cilium, consultez le **guide complet de dÃ©ploiement** :

ğŸ“– **[Guide de dÃ©ploiement Talos sur Outscale](https://blog.stephane-robert.info/docs/cloud/outscale/kubernetes-talos/)**

Ce guide dÃ©taille :

- La gÃ©nÃ©ration des configurations Talos
- Le bootstrap du cluster etcd
- L'installation et la configuration de Cilium CNI
- Les tests de connectivitÃ©
- Le troubleshooting

ğŸ‰ **Votre cluster Kubernetes Talos sera opÃ©rationnel !**

## ğŸ“ Structure du projet

```text
.
â”œâ”€â”€ README.md                      # Ce fichier
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ PROCEDURE-IMAGE-TALOS-GPU.md  # Guide dÃ©taillÃ© crÃ©ation images GPU
â”œâ”€â”€ .envrc.sample                  # Template de credentials
â”œâ”€â”€ cilium-patch.yaml              # Patch Talos pour dÃ©sactiver kube-proxy
â”œâ”€â”€ packer/
â”‚   â”œâ”€â”€ talos-outscale.pkr.hcl        # Configuration Packer image standard
â”‚   â”œâ”€â”€ talos-gpu-outscale.pkr.hcl    # Configuration Packer image GPU
â”‚   â”œâ”€â”€ variables.auto.pkrvars.hcl.example  # Variables Packer
â”‚   â”œâ”€â”€ manifest.json                 # Manifest build image standard
â”‚   â”œâ”€â”€ manifest-gpu-universal.json   # Manifest build image GPU
â”‚   â””â”€â”€ provision/
â”‚       â”œâ”€â”€ playbook.yaml          # Playbook Ansible image standard
â”‚       â”œâ”€â”€ playbook-gpu.yaml      # Playbook Ansible image GPU
â”‚       â””â”€â”€ schematic.yaml         # Schematic Talos (extensions)
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf                    # Configuration Terraform principale
â”‚   â”œâ”€â”€ variables.tf               # Variables (inclut GPU)
â”‚   â”œâ”€â”€ outputs.tf                 # Outputs (inclut GPU workers)
â”‚   â”œâ”€â”€ network.tf                 # Configuration rÃ©seau
â”‚   â”œâ”€â”€ compute.tf                 # VMs Talos (CP, workers, GPU workers)
â”‚   â”œâ”€â”€ security_groups.tf         # Security Groups
â”‚   â”œâ”€â”€ load_balancer.tf           # Load Balancer API Kubernetes
â”‚   â”œâ”€â”€ keypair.tf                 # Paire de clÃ©s SSH
â”‚   â”œâ”€â”€ terraform.tfvars.example   # Exemple de variables
â”‚   â”œâ”€â”€ deploy-cluster.sh          # Script automatisÃ© dÃ©ploiement complet
â”‚   â”œâ”€â”€ generate-talos-config.sh   # Script gÃ©nÃ©ration configs Talos
â”‚   â””â”€â”€ talos-patches/
â”‚       â””â”€â”€ gpu-worker-patch.yaml  # Patch Talos pour workers GPU
â”œâ”€â”€ kubernetes/
â”‚   â”œâ”€â”€ storageclass-outscale.yaml    # StorageClass CSI Outscale
â”‚   â”œâ”€â”€ test-csi-pvc.yaml             # Tests PVC
â”‚   â”œâ”€â”€ test-gpu-pod.yaml             # Tests GPU
â”‚   â”œâ”€â”€ install-gpu-operator.sh       # Installation GPU Operator
â”‚   â””â”€â”€ setup-gpu-node.sh             # Configuration nÅ“ud GPU
â””â”€â”€ _out/                          # Outputs gÃ©nÃ©rÃ©s
    â”œâ”€â”€ talosconfig
    â”œâ”€â”€ kubeconfig
    â”œâ”€â”€ controlplane.yaml
    â”œâ”€â”€ worker.yaml
    â”œâ”€â”€ gpu-worker.yaml            # Config worker GPU
    â”œâ”€â”€ gpu-operator-values-clean.yaml  # Values GPU Operator
    â””â”€â”€ GPU-OPERATOR-INSTALL-GUIDE.md   # Guide GPU Operator
```

## ğŸ“š Documentation

Documentation disponible :

- **Guide complet de dÃ©ploiement** : [DÃ©ploiement Talos sur Outscale](https://blog.stephane-robert.info/docs/cloud/outscale/kubernetes-talos/)

Ressources externes :

- **Talos officiel** : <https://www.talos.dev/>
- **Cilium** : <https://docs.cilium.io/>
- **Terraform Outscale** : <https://registry.terraform.io/providers/outscale/outscale/>

### FonctionnalitÃ©s principales

#### Support GPU NVIDIA

Le projet supporte le dÃ©ploiement de workers GPU avec :

- **Images personnalisÃ©es** : OMI Talos avec drivers NVIDIA prÃ©-installÃ©s
- **Extensions Talos** : nvidia-open-gpu-kernel-modules, nvidia-container-toolkit, nvidia-fabricmanager
- **Drivers** : NVIDIA 570.x (production)
- **Configurations** : GPU simple, multi-GPU, et systÃ¨mes HGX avec NVLink
- **GPU Operator** : Installation et gestion automatisÃ©es des composants GPU

#### CSI Driver Outscale

Support du stockage persistant avec volumes BSU :

- **StorageClass** : gp2, io1, standard
- **Dynamic provisioning** : CrÃ©ation automatique de volumes
- **Volume expansion** : Redimensionnement Ã  chaud
- **Snapshots** : Sauvegarde et restauration

#### Cloud Controller Manager

IntÃ©gration native avec les services Outscale :

- **Load Balancers** : CrÃ©ation automatique de LB pour les Services Kubernetes
- **Node management** : Synchronisation des mÃ©tadonnÃ©es VM/Node
- **Zone awareness** : Distribution multi-AZ intelligente

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :

1. Forker le projet
2. CrÃ©er une branche feature (`git checkout -b feature/AmazingFeature`)
3. Commiter vos changements (`git commit -m 'Add some AmazingFeature'`)
4. Pousser vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

### Guidelines

- Respectez la structure du projet
- Documentez les nouveaux composants
- Testez sur une infrastructure de dev avant de proposer
- Mettez Ã  jour le README si nÃ©cessaire

## ğŸ“ Licence

Ce projet est distribuÃ© sous licence Apache 2.0. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

## ğŸ‘¤ Auteur

**StÃ©phane Robert**

- Blog: <https://blog.stephane-robert.info>
- GitHub: [@stephrobert](https://github.com/stephrobert)
- LinkedIn: [StÃ©phane Robert](https://www.linkedin.com/in/stephanerobert1/)

---

â­ **Si ce projet vous est utile, n'hÃ©sitez pas Ã  lui mettre une star !**
